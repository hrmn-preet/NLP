{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog sat on the log.\",\n",
    "    \"Cats and dogs are great pets.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting and transforming\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.42755362 0.         0.         0.\n",
      "  0.         0.         0.42755362 0.32516555 0.         0.32516555\n",
      "  0.6503311 ]\n",
      " [0.         0.         0.         0.         0.42755362 0.\n",
      "  0.         0.42755362 0.         0.32516555 0.         0.32516555\n",
      "  0.6503311 ]\n",
      " [0.40824829 0.40824829 0.         0.40824829 0.         0.40824829\n",
      "  0.40824829 0.         0.         0.         0.40824829 0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#converting to dense format and printing\n",
    "dense_matrix = tfidf_matrix.todense()\n",
    "print(dense_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'are' 'cat' 'cats' 'dog' 'dogs' 'great' 'log' 'mat' 'on' 'pets'\n",
      " 'sat' 'the']\n"
     ]
    }
   ],
   "source": [
    "#getting the feature name\n",
    "features_names = vectorizer.get_feature_names_out()\n",
    "print(features_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer Matrix:\n",
      " [[0 0 1 0 0 0 0 0 1 1 0 1 2]\n",
      " [0 0 0 0 1 0 0 1 0 1 0 1 2]\n",
      " [1 1 0 1 0 1 1 0 0 0 1 0 0]]\n",
      "Feature Names:\n",
      " ['and' 'are' 'cat' 'cats' 'dog' 'dogs' 'great' 'log' 'mat' 'on' 'pets'\n",
      " 'sat' 'the']\n",
      "TfidfVectorizer Matrix:\n",
      " [[0.         0.         0.42755362 0.         0.         0.\n",
      "  0.         0.         0.42755362 0.32516555 0.         0.32516555\n",
      "  0.6503311 ]\n",
      " [0.         0.         0.         0.         0.42755362 0.\n",
      "  0.         0.42755362 0.         0.32516555 0.         0.32516555\n",
      "  0.6503311 ]\n",
      " [0.40824829 0.40824829 0.         0.40824829 0.         0.40824829\n",
      "  0.40824829 0.         0.         0.         0.40824829 0.\n",
      "  0.        ]]\n",
      "['and' 'are' 'cat' 'cats' 'dog' 'dogs' 'great' 'log' 'mat' 'on' 'pets'\n",
      " 'sat' 'the']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog sat on the log.\",\n",
    "    \"Cats and dogs are great pets.\"\n",
    "]\n",
    "\n",
    "# CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix = count_vectorizer.fit_transform(documents)\n",
    "print(\"CountVectorizer Matrix:\\n\", count_matrix.toarray())\n",
    "print(\"Feature Names:\\n\", count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "print(\"TfidfVectorizer Matrix:\\n\", tfidf_matrix.toarray())\n",
    "features_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(features_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I instantiate a TfidfVectorizer with the following parameters:\n",
    "- max_df = 0.95\n",
    "- min_df = 2\n",
    "- max_features = no_features\n",
    "- stop_words = 'english'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df = 0.95\n",
    "min_df = 2\n",
    "max_features = None\n",
    "stop_words = 'english'\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=max_df,\n",
    "    min_df=min_df,\n",
    "    max_features=max_features,\n",
    "    stop_words=stop_words\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of Parameters\n",
    "- max_df = 0.95: Ignore terms that appear in more than 95% of the documents. This helps to remove extremely common words that are not informative.\n",
    "- min_df = 2: Ignore terms that appear in fewer than 2 documents. This helps to remove very rare words that might not be useful for the analysis.\n",
    "- max_features = None: This parameter can be set to a specific number to limit the number of features (terms) to be considered. Setting it to None means there is no limit on the number of features.\n",
    "- stop_words = 'english': Use a built-in list of English stop words to exclude from the analysis. These are common words like \"and\", \"the\", etc., which typically do not carry meaningful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "['sat']\n"
     ]
    }
   ],
   "source": [
    "# Example usage with some documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog sat on the log.\",\n",
    "    \"Cats and dogs are great pets.\"\n",
    "]\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the matrix to a dense format and print\n",
    "dense_matrix = tfidf_matrix.todense()\n",
    "print(dense_matrix)\n",
    "\n",
    "# Get the feature names (terms)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vectorizer objects methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      " [[1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Feature Names:\n",
      " ['sat']\n",
      "Stop Words:\n",
      " frozenset({'fire', 'couldnt', 'nowhere', 'before', 'thus', 'together', 'must', 'latter', 'or', 'thereafter', 'among', 'indeed', 'elsewhere', 'sixty', 'still', 'no', 'third', 'upon', 'thereupon', 'whence', 'thick', 'your', 'six', 'had', 'whenever', 'do', 'get', 'why', 'my', 'somehow', 'fifteen', 'becomes', 'over', 'mill', 'at', 'they', 'more', 'for', 'few', 'which', 'interest', 'these', 'the', 'perhaps', 'therefore', 'will', 'have', 'amount', 'co', 'us', 'may', 'eleven', 'everything', 'neither', 'down', 'further', 'beyond', 'being', 'moreover', 'whereafter', 'yourself', 'i', 'per', 'can', 'hereby', 'be', 'nobody', 'nine', 'amongst', 'cry', 'nevertheless', 'somewhere', 'even', 'this', 'however', 'themselves', 'very', 'though', 'a', 'else', 'bottom', 'although', 'he', 'always', 'least', 'am', 'never', 'four', 'it', 'every', 'one', 'whose', 'afterwards', 'eg', 'something', 'often', 'others', 'any', 'myself', 'ours', 'was', 'made', 'hereupon', 'therein', 'mine', 'along', 'becoming', 'sometime', 'also', 'latterly', 'move', 'twelve', 'when', 'their', 'herself', 'up', 'twenty', 'wherein', 'who', 'anyhow', 'found', 'yet', 'whether', 'enough', 'un', 'alone', 'so', 'detail', 'many', 'meanwhile', 'herein', 'whoever', 'fill', 'except', 'whom', 'everyone', 'nothing', 'already', 'most', 'an', 'across', 'bill', 're', 'ie', 'thru', 'has', 'were', 'without', 'namely', 'beforehand', 'each', 'ever', 'ourselves', 'ltd', 'her', 'above', 'in', 'beside', 'onto', 'since', 'there', 'what', 'about', 'and', 'toward', 'both', 'wherever', 'how', 'thin', 'via', 'either', 'while', 'whereby', 'ten', 'towards', 'hence', 'on', 'part', 'find', 'itself', 'yourselves', 'whither', 'back', 'formerly', 'into', 'if', 'might', 'thence', 'well', 'him', 'she', 'until', 'next', 'another', 'our', 'as', 'two', 'seeming', 'you', 'otherwise', 'front', 'hasnt', 'show', 'keep', 'below', 'whatever', 'full', 'whole', 'much', 'them', 'became', 'is', 'once', 'someone', 'seems', 'serious', 'go', 'me', 'of', 'take', 'seem', 'due', 'first', 'none', 'its', 'several', 'same', 'anything', 'behind', 'three', 'con', 'again', 'by', 'five', 'through', 'call', 'etc', 'out', 'all', 'please', 'eight', 'other', 'after', 'less', 'from', 'his', 'some', 'should', 'hundred', 'such', 'we', 'whereupon', 'side', 'cannot', 'thereby', 'own', 'could', 'anywhere', 'nor', 'mostly', 'against', 'sometimes', 'seemed', 'around', 'now', 'too', 'top', 'name', 'become', 'throughout', 'besides', 'but', 'where', 'been', 'then', 'everywhere', 'give', 'yours', 'under', 'hereafter', 'cant', 'rather', 'not', 'would', 'hers', 'amoungst', 'during', 'that', 'anyway', 'because', 'put', 'are', 'forty', 'off', 'empty', 'anyone', 'only', 'system', 'last', 'former', 'see', 'within', 'sincere', 'between', 'whereas', 'here', 'de', 'done', 'inc', 'fifty', 'noone', 'than', 'to', 'himself', 'almost', 'describe', 'those', 'with'})\n",
      "Parameters:\n",
      " {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.float64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 0.95, 'max_features': None, 'min_df': 2, 'ngram_range': (1, 1), 'norm': 'l2', 'preprocessor': None, 'smooth_idf': True, 'stop_words': 'english', 'strip_accents': None, 'sublinear_tf': False, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None, 'use_idf': True, 'vocabulary': None}\n",
      "Updated Parameters:\n",
      " {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.float64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 0.95, 'max_features': 10, 'min_df': 2, 'ngram_range': (1, 1), 'norm': 'l2', 'preprocessor': None, 'smooth_idf': True, 'stop_words': 'english', 'strip_accents': None, 'sublinear_tf': False, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None, 'use_idf': True, 'vocabulary': None}\n",
      "New TF-IDF Matrix:\n",
      " [[1.]\n",
      " [0.]]\n",
      "Inverse Transformed:\n",
      " [array(['sat'], dtype='<U3'), array([], dtype='<U3')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog sat on the log.\",\n",
    "    \"Cats and dogs are great pets.\"\n",
    "]\n",
    "\n",
    "# Instantiate TfidfVectorizer with specific parameters\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the matrix to a dense format\n",
    "dense_matrix = tfidf_matrix.todense()\n",
    "print(\"TF-IDF Matrix:\\n\", dense_matrix)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Feature Names:\\n\", feature_names)\n",
    "\n",
    "# Get stop words\n",
    "stop_words = vectorizer.get_stop_words()\n",
    "print(\"Stop Words:\\n\", stop_words)\n",
    "\n",
    "# Get parameters\n",
    "params = vectorizer.get_params()\n",
    "print(\"Parameters:\\n\", params)\n",
    "\n",
    "# Set new parameters\n",
    "vectorizer.set_params(max_features=10)\n",
    "print(\"Updated Parameters:\\n\", vectorizer.get_params())\n",
    "\n",
    "# Transform new documents using the fitted vectorizer\n",
    "new_documents = [\n",
    "    \"The cat and the dog sat on the log.\",\n",
    "    \"Great pets are cats and dogs.\"\n",
    "]\n",
    "new_tfidf_matrix = vectorizer.transform(new_documents)\n",
    "new_dense_matrix = new_tfidf_matrix.todense()\n",
    "print(\"New TF-IDF Matrix:\\n\", new_dense_matrix)\n",
    "\n",
    "# Inverse transform the matrix\n",
    "inverse_transformed = vectorizer.inverse_transform(new_tfidf_matrix)\n",
    "print(\"Inverse Transformed:\\n\", inverse_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-negative Matrix Factorization (NMF) is a group of algorithms in linear algebra where a given matrix \n",
    "ð‘‰\n",
    "V is factorized into (usually) two matrices \n",
    "ð‘Š\n",
    "W and \n",
    "ð»\n",
    "H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect, making NMF useful for applications where interpretability is important.\n",
    "\n",
    "Key Concepts and Definitions\n",
    "Non-negative Matrix: A matrix in which all the elements are zero or positive.\n",
    "Factorization: The process of decomposing a matrix into two or more matrices that, when multiplied together, approximate the original matrix.\n",
    "Given a matrix \n",
    "ð‘‰\n",
    "V of size \n",
    "ð‘š\n",
    "Ã—\n",
    "ð‘›\n",
    "mÃ—n:\n",
    "\n",
    "ð‘‰\n",
    "â‰ˆ\n",
    "ð‘Š\n",
    "ð»\n",
    "Vâ‰ˆWH\n",
    "\n",
    "where:\n",
    "\n",
    "ð‘Š\n",
    "W is an \n",
    "ð‘š\n",
    "Ã—\n",
    "ð‘Ÿ\n",
    "mÃ—r matrix (usually called the basis matrix),\n",
    "ð»\n",
    "H is an \n",
    "ð‘Ÿ\n",
    "Ã—\n",
    "ð‘›\n",
    "rÃ—n matrix (usually called the coefficient matrix),\n",
    "ð‘Ÿ\n",
    "r is the rank of the factorization, and \n",
    "ð‘Ÿ\n",
    "r is chosen to be less than both \n",
    "ð‘š\n",
    "m and \n",
    "ð‘›\n",
    "n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "dogs cats pets loyal great\n",
      "Topic 1\n",
      "sat mat cat log dog\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog sat on the log.\",\n",
    "    \"Cats and dogs are great pets.\",\n",
    "    \"Pets can be very friendly and loyal.\",\n",
    "    \"Cats are more independent than dogs.\",\n",
    "    \"Dogs are known to be loyal and protective.\"\n",
    "]\n",
    "\n",
    "# Convert documents to TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "#apply NF\n",
    "num_topics = 2\n",
    "nmf_model = NMF(n_components=num_topics, random_state=42)\n",
    "W = nmf_model.fit_transform(tfidf_matrix)\n",
    "H = nmf_model.components_\n",
    "\n",
    "#displaying the topis as H matrix has the topics and W has the relationships between document and topics\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(H):\n",
    "    print(f\"Topic {topic_idx}\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-6:-1]]))\n",
    "\n",
    "#displaying the document topic matrix\n",
    "#print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "dogs cats great known protective\n",
      "Topic 1:\n",
      "sat loyal pets mat cat\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog sat on the log.\",\n",
    "    \"Cats and dogs are great pets.\",\n",
    "    \"Pets can be very friendly and loyal.\",\n",
    "    \"Cats are more independent than dogs.\",\n",
    "    \"Dogs are known to be loyal and protective.\"\n",
    "]\n",
    "\n",
    "# Convert documents to a matrix of token counts\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Apply LDA\n",
    "num_topics = 2\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda_model.fit(X)\n",
    "\n",
    "# Get the top words for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    print(f\"Topic {topic_idx}:\")\n",
    "    top_words_idx = topic.argsort()[:-6:-1]  # Get indices of top 5 words\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(\" \".join(top_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all this to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_list):  \n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        top_indices = topic.argsort()[:-6:-1]\n",
    "        top_words = [feature_list[i] for i in top_indices]\n",
    "        print(\" \".join(top_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "#import getch_20news groups from sklear.datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "#import NMF and LDA from sklearn\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers','footers','quotes'))\n",
    "\n",
    "documents = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_features = 100\n",
    "no_topics = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_df = 0.95,\n",
    "    min_df = 2,\n",
    "    max_features = no_features,\n",
    "    stop_words = 'english'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '10' '12' '14' '15' '16' '20' '25' 'a86' 'available' 'ax' 'b8f'\n",
      " 'believe' 'best' 'better' 'bit' 'case' 'com' 'come' 'course' 'data' 'day'\n",
      " 'did' 'didn' 'different' 'does' 'doesn' 'don' 'drive' 'edu' 'fact' 'far'\n",
      " 'file' 'g9v' 'god' 'going' 'good' 'got' 'government' 'help' 'information'\n",
      " 'jesus' 'just' 'key' 'know' 'law' 'let' 'like' 'line' 'list' 'little'\n",
      " 'll' 'long' 'look' 'lot' 'mail' 'make' 'max' 'mr' 'need' 'new' 'number'\n",
      " 'people' 'point' 'power' 'probably' 'problem' 'program' 'question' 'read'\n",
      " 'really' 'right' 'run' 'said' 'say' 'second' 'set' 'software' 'space'\n",
      " 'state' 'sure' 'tell' 'thanks' 'thing' 'things' 'think' 'time' 'true'\n",
      " 'try' 'use' 'used' 'using' 've' 'want' 'way' 'windows' 'work' 'world'\n",
      " 'year' 'years']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(n_components=no_topics,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [2.34138105e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.15384918e-01],\n",
       "       [1.16885598e-08, 0.00000000e+00, 2.47453911e-02, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 8.05880091e-21, 4.92613667e-21],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.18602982e-02, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_model.fit_transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 5.63513917e-06]\n",
      " [5.87119927e-08 0.00000000e+00 7.70919213e-09 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 7.20330403e-10 ... 0.00000000e+00\n",
      "  1.24898125e-15 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "H = nmf_model.components_\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "people know mr 14 different\n",
      "Topic 1:\n",
      "does know 14 set different\n",
      "Topic 2:\n",
      "know does read question didn\n",
      "Topic 3:\n",
      "edu 14 file mr set\n",
      "Topic 4:\n",
      "just a86 things don years\n",
      "Topic 5:\n",
      "like mr read different 25\n",
      "Topic 6:\n",
      "just years good doesn don\n",
      "Topic 7:\n",
      "use max set don read\n",
      "Topic 8:\n",
      "thanks max set read file\n",
      "Topic 9:\n",
      "good mr different read let\n",
      "Topic 10:\n",
      "think don set read question\n",
      "Topic 11:\n",
      "god things don mr jesus\n",
      "Topic 12:\n",
      "problem 14 file question didn\n",
      "Topic 13:\n",
      "windows read set different 25\n",
      "Topic 14:\n",
      "drive max different mr set\n",
      "Topic 15:\n",
      "time max ll 10 let\n",
      "Topic 16:\n",
      "don different ll case question\n",
      "Topic 17:\n",
      "ve question didn going doesn\n",
      "Topic 18:\n",
      "bit max 10 file let\n",
      "Topic 19:\n",
      "com max mr 25 file\n",
      "Topic 20:\n",
      "need max 10 file ll\n",
      "Topic 21:\n",
      "used available file question years\n",
      "Topic 22:\n",
      "year 14 10 ll new\n",
      "Topic 23:\n",
      "right 10 question years going\n",
      "Topic 24:\n",
      "key make 14 want space\n",
      "Topic 25:\n",
      "make 14 ll let question\n",
      "Topic 26:\n",
      "mail file available let ll\n",
      "Topic 27:\n",
      "way ll let years going\n",
      "Topic 28:\n",
      "things say mr going let\n",
      "Topic 29:\n",
      "10 b8f a86 question years\n",
      "Topic 30:\n",
      "want max mr different make\n",
      "Topic 31:\n",
      "really mr read 10 didn\n",
      "Topic 32:\n",
      "said mr 25 let file\n",
      "Topic 33:\n",
      "say just ll different let\n",
      "Topic 34:\n",
      "space max 14 file different\n",
      "Topic 35:\n",
      "did mr just set 25\n",
      "Topic 36:\n",
      "work max just 14 different\n",
      "Topic 37:\n",
      "file a86 g9v max 25\n",
      "Topic 38:\n",
      "believe just file 10 going\n",
      "Topic 39:\n",
      "got 14 file ll let\n",
      "Topic 40:\n",
      "using 14 file 10 ll\n",
      "Topic 41:\n",
      "sure just ll make file\n",
      "Topic 42:\n",
      "question just ll 10 going\n",
      "Topic 43:\n",
      "course little years mr use\n",
      "Topic 44:\n",
      "going just ll let got\n",
      "Topic 45:\n",
      "ll b8f g9v just going\n",
      "Topic 46:\n",
      "probably just ll let different\n",
      "Topic 47:\n",
      "years max 10 ll let\n",
      "Topic 48:\n",
      "program 14 know file let\n",
      "Topic 49:\n",
      "help know mr file 25\n",
      "Topic 50:\n",
      "true max set just read\n",
      "Topic 51:\n",
      "know problem help 14 just\n",
      "Topic 52:\n",
      "better max a86 just ll\n",
      "Topic 53:\n",
      "case just ll years going\n",
      "Topic 54:\n",
      "software 14 set does available\n",
      "Topic 55:\n",
      "information max 14 file mr\n",
      "Topic 56:\n",
      "best 10 world going didn\n",
      "Topic 57:\n",
      "number 14 file available 10\n",
      "Topic 58:\n",
      "read ax just file ll\n",
      "Topic 59:\n",
      "doesn just mr different read\n",
      "Topic 60:\n",
      "didn just read ll different\n",
      "Topic 61:\n",
      "power max 14 set 10\n",
      "Topic 62:\n",
      "list 14 file 10 different\n",
      "Topic 63:\n",
      "data ax 14 just file\n",
      "Topic 64:\n",
      "tell just ll 10 set\n",
      "Topic 65:\n",
      "let a86 g9v just ll\n",
      "Topic 66:\n",
      "world years 14 little better\n",
      "Topic 67:\n",
      "government people little point years\n",
      "Topic 68:\n",
      "lot just people different read\n",
      "Topic 69:\n",
      "00 14 set make good\n",
      "Topic 70:\n",
      "look mr just file like\n",
      "Topic 71:\n",
      "try max just file 10\n",
      "Topic 72:\n",
      "long max just 10 far\n",
      "Topic 73:\n",
      "law 14 people help just\n",
      "Topic 74:\n",
      "available a86 g9v different key\n",
      "Topic 75:\n",
      "set ax 14 file just\n",
      "Topic 76:\n",
      "day mr just read jesus\n",
      "Topic 77:\n",
      "thing max just people read\n",
      "Topic 78:\n",
      "line file just 10 let\n",
      "Topic 79:\n",
      "run max just file 10\n",
      "Topic 80:\n",
      "little max just mr ll\n",
      "Topic 81:\n",
      "state max people mr just\n",
      "Topic 82:\n",
      "second 14 people just 10\n",
      "Topic 83:\n",
      "fact just people 14 little\n",
      "Topic 84:\n",
      "different ax just time little\n",
      "Topic 85:\n",
      "come 14 people mr just\n",
      "Topic 86:\n",
      "far people just mr let\n",
      "Topic 87:\n",
      "jesus say god point 14\n",
      "Topic 88:\n",
      "12 14 10 25 just\n",
      "Topic 89:\n",
      "mr ax b8f g9v a86\n",
      "Topic 90:\n",
      "20 14 god 10 day\n",
      "Topic 91:\n",
      "15 max 14 say 10\n",
      "Topic 92:\n",
      "16 max 14 10 god\n",
      "Topic 93:\n",
      "25 ax b8f 16 g9v\n",
      "Topic 94:\n",
      "14 a86 b8f g9v max\n",
      "Topic 95:\n",
      "point max say people just\n",
      "Topic 96:\n",
      "max ax b8f a86 g9v\n",
      "Topic 97:\n",
      "key say 16 use little\n",
      "Topic 98:\n",
      "bit course things say make\n",
      "Topic 99:\n",
      "new b8f max say people\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_model,feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=2,\n",
    "    max_features=no_features,\n",
    "    stop_words='english'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix = count_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '10' '12' '14' '15' '16' '20' '25' 'a86' 'available' 'ax' 'b8f'\n",
      " 'believe' 'best' 'better' 'bit' 'case' 'com' 'come' 'course' 'data' 'day'\n",
      " 'did' 'didn' 'different' 'does' 'doesn' 'don' 'drive' 'edu' 'fact' 'far'\n",
      " 'file' 'g9v' 'god' 'going' 'good' 'got' 'government' 'help' 'information'\n",
      " 'jesus' 'just' 'key' 'know' 'law' 'let' 'like' 'line' 'list' 'little'\n",
      " 'll' 'long' 'look' 'lot' 'mail' 'make' 'max' 'mr' 'need' 'new' 'number'\n",
      " 'people' 'point' 'power' 'probably' 'problem' 'program' 'question' 'read'\n",
      " 'really' 'right' 'run' 'said' 'say' 'second' 'set' 'software' 'space'\n",
      " 'state' 'sure' 'tell' 'thanks' 'thing' 'things' 'think' 'time' 'true'\n",
      " 'try' 'use' 'used' 'using' 've' 'want' 'way' 'windows' 'work' 'world'\n",
      " 'year' 'years']\n"
     ]
    }
   ],
   "source": [
    "#getting the feature name\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(\n",
    "    n_components=no_topics,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.001     , 0.001     , 0.001     , ..., 0.001     , 0.001     ,\n",
       "        0.001     ],\n",
       "       [0.00111111, 0.00111111, 0.00111111, ..., 0.00111111, 0.00111111,\n",
       "        0.00111111],\n",
       "       [0.00111111, 0.00111111, 0.00111111, ..., 0.00111111, 0.00111111,\n",
       "        0.00111111],\n",
       "       ...,\n",
       "       [0.00333333, 0.00333333, 0.00333333, ..., 0.00333333, 0.00333333,\n",
       "        0.00333333],\n",
       "       [0.002     , 0.002     , 0.002     , ..., 0.002     , 0.19482313,\n",
       "        0.002     ],\n",
       "       [0.0004    , 0.0004    , 0.0004    , ..., 0.0004    , 0.04021966,\n",
       "        0.0004    ]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit_transform(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "jesus god know way said\n",
      "Topic 1:\n",
      "edu mail like just people\n",
      "Topic 2:\n",
      "data use way just like\n",
      "Topic 3:\n",
      "jesus people come does make\n",
      "Topic 4:\n",
      "long just way time like\n",
      "Topic 5:\n",
      "ax max g9v b8f 25\n",
      "Topic 6:\n",
      "line ll just look like\n",
      "Topic 7:\n",
      "jesus god say did don\n",
      "Topic 8:\n",
      "mr going know don think\n",
      "Topic 9:\n",
      "law fact does people way\n",
      "Topic 10:\n",
      "ax b8f a86 g9v max\n",
      "Topic 11:\n",
      "state don say better way\n",
      "Topic 12:\n",
      "10 20 15 14 25\n",
      "Topic 13:\n",
      "best better good way probably\n",
      "Topic 14:\n",
      "key use like using don\n",
      "Topic 15:\n",
      "world better 20 information new\n",
      "Topic 16:\n",
      "right just way people like\n",
      "Topic 17:\n",
      "different like good just ll\n",
      "Topic 18:\n",
      "think don just know good\n",
      "Topic 19:\n",
      "day people way going good\n",
      "Topic 20:\n",
      "just going say good way\n",
      "Topic 21:\n",
      "bit 16 better max way\n",
      "Topic 22:\n",
      "file windows just use 20\n",
      "Topic 23:\n",
      "little just better good don\n",
      "Topic 24:\n",
      "use way does used know\n",
      "Topic 25:\n",
      "case way better like think\n",
      "Topic 26:\n",
      "people just don say government\n",
      "Topic 27:\n",
      "00 new don help ve\n",
      "Topic 28:\n",
      "key bit number used information\n",
      "Topic 29:\n",
      "ax max b8f a86 mr\n",
      "Topic 30:\n",
      "thanks know does windows way\n",
      "Topic 31:\n",
      "don know ll probably going\n",
      "Topic 32:\n",
      "space 10 program 14 15\n",
      "Topic 33:\n",
      "doesn ve don just know\n",
      "Topic 34:\n",
      "make better say don way\n",
      "Topic 35:\n",
      "power doesn going just like\n",
      "Topic 36:\n",
      "need know don ll way\n",
      "Topic 37:\n",
      "12 16 10 14 15\n",
      "Topic 38:\n",
      "think people better going like\n",
      "Topic 39:\n",
      "file program line read year\n",
      "Topic 40:\n",
      "true way does people say\n",
      "Topic 41:\n",
      "drive 16 way just use\n",
      "Topic 42:\n",
      "try don better just way\n",
      "Topic 43:\n",
      "point better use like doesn\n",
      "Topic 44:\n",
      "question way know don probably\n",
      "Topic 45:\n",
      "want don ll know just\n",
      "Topic 46:\n",
      "said people say way time\n",
      "Topic 47:\n",
      "new like just ll going\n",
      "Topic 48:\n",
      "list space far data time\n",
      "Topic 49:\n",
      "know like don does doesn\n",
      "Topic 50:\n",
      "course probably way ll better\n",
      "Topic 51:\n",
      "way world time people point\n",
      "Topic 52:\n",
      "data software available information edu\n",
      "Topic 53:\n",
      "ll going said say know\n",
      "Topic 54:\n",
      "let know don say better\n",
      "Topic 55:\n",
      "ve going work new make\n",
      "Topic 56:\n",
      "lot like just better don\n",
      "Topic 57:\n",
      "second time just don 15\n",
      "Topic 58:\n",
      "available use probably know like\n",
      "Topic 59:\n",
      "did way know say just\n",
      "Topic 60:\n",
      "good 25 ll like just\n",
      "Topic 61:\n",
      "thing way like good just\n",
      "Topic 62:\n",
      "sure just way like make\n",
      "Topic 63:\n",
      "read doesn just know time\n",
      "Topic 64:\n",
      "come people think going just\n",
      "Topic 65:\n",
      "windows use ll better don\n",
      "Topic 66:\n",
      "got just going ve way\n",
      "Topic 67:\n",
      "didn know don ll time\n",
      "Topic 68:\n",
      "time ll going know good\n",
      "Topic 69:\n",
      "like just going don doesn\n",
      "Topic 70:\n",
      "set way ll use like\n",
      "Topic 71:\n",
      "fact way say doesn time\n",
      "Topic 72:\n",
      "years time like say people\n",
      "Topic 73:\n",
      "year years going ll just\n",
      "Topic 74:\n",
      "program know like way better\n",
      "Topic 75:\n",
      "run going like time just\n",
      "Topic 76:\n",
      "mail list ll like know\n",
      "Topic 77:\n",
      "look just like good know\n",
      "Topic 78:\n",
      "work doesn just don way\n",
      "Topic 79:\n",
      "file ll like want right\n",
      "Topic 80:\n",
      "far know going say way\n",
      "Topic 81:\n",
      "help does know thanks like\n",
      "Topic 82:\n",
      "com edu don list just\n",
      "Topic 83:\n",
      "number 10 use 20 doesn\n",
      "Topic 84:\n",
      "ve just going way time\n",
      "Topic 85:\n",
      "say believe good think like\n",
      "Topic 86:\n",
      "government fact use new used\n",
      "Topic 87:\n",
      "things don know way just\n",
      "Topic 88:\n",
      "available use edu program information\n",
      "Topic 89:\n",
      "information mail know people use\n",
      "Topic 90:\n",
      "really doesn going don just\n",
      "Topic 91:\n",
      "believe don people know going\n",
      "Topic 92:\n",
      "god does say people know\n",
      "Topic 93:\n",
      "software does know like don\n",
      "Topic 94:\n",
      "using way does use just\n",
      "Topic 95:\n",
      "tell just don better say\n",
      "Topic 96:\n",
      "does just know ll better\n",
      "Topic 97:\n",
      "better year good don think\n",
      "Topic 98:\n",
      "used way better use don\n",
      "Topic 99:\n",
      "problem know just going time\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda,feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim and LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents from above exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize documents\n",
    "tokenized_docs = [doc.split() for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dictionary of documents\n",
    "dictionary = corpora.Dictionary(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting tokenized docs into a document topic matrix\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.019*\"X\" + 0.008*\"-\" + '\n",
      "  '0.008*\"MAX>\\'AX>\\'AX>\\'AX>\\'AX>\\'AX>\\'AX>\\'AX>\\'AX>\\'AX>\\'AX>\\'AX>\\'AX>\\'AX>\\'AX>\\'\" '\n",
      "  '+ 0.008*\"1\" + 0.005*\"0\" + 0.005*\"2\" + 0.005*\"*\" + 0.004*\"for\" + 0.003*\"and\" '\n",
      "  '+ 0.003*\"*/\"'),\n",
      " (1,\n",
      "  '0.050*\"the\" + 0.027*\"to\" + 0.024*\"of\" + 0.021*\"a\" + 0.020*\"and\" + '\n",
      "  '0.015*\"is\" + 0.015*\"in\" + 0.015*\"I\" + 0.013*\"that\" + 0.009*\"for\"')]\n"
     ]
    }
   ],
   "source": [
    "#traing LDA model\n",
    "num_topics = 2\n",
    "lda_model = LdaModel(corpus, num_topics=num_topics,id2word=dictionary,passes=10)\n",
    "\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "THE_ONE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
